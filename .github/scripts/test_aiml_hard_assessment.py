"""
AWS AI/ML Services - Hard Assessment - Test Script
Tests SageMaker Pipelines, Batch Transform, Model Monitoring, MLOps
"""

import sys
import os
import boto3
from botocore.exceptions import ClientError

def get_student_id():
    """Get student ID from command line arguments"""
    if len(sys.argv) < 2:
        print("Error: Student ID is required")
        print("Usage: python validate_tasks.py aiml_hard")
        sys.exit(1)
    return sys.argv[1]

def test_pipeline_exists(sagemaker_client, student_id):
    """Task 1: Verify SageMaker Pipeline exists"""
    try:
        response = sagemaker_client.list_pipelines(
            PipelineNamePrefix=f'MLPipeline-{student_id}',
            MaxResults=10
        )
        pipelines = response.get('PipelineSummaries', [])
        
        if not pipelines:
            print(f"âš ï¸  Task 1 Warning: No pipelines found")
            return True
        
        # Check pipeline executions
        pipeline_name = pipelines[0]['PipelineName']
        executions = sagemaker_client.list_pipeline_executions(
            PipelineName=pipeline_name,
            MaxResults=5
        )
        
        execution_count = len(executions.get('PipelineExecutionSummaries', []))
        
        print(f"âœ… Task 1 Passed: Pipeline '{pipeline_name}' with {execution_count} execution(s)")
        return True
    except Exception as e:
        print(f"âš ï¸  Task 1 Warning: {e}")
        return True

def test_tuning_job_exists(sagemaker_client, student_id):
    """Task 2: Verify hyperparameter tuning job"""
    try:
        response = sagemaker_client.list_hyper_parameter_tuning_jobs(
            NameContains=student_id,
            MaxResults=10,
            SortBy='CreationTime',
            SortOrder='Descending'
        )
        jobs = response.get('HyperParameterTuningJobSummaries', [])
        
        if not jobs:
            print(f"âš ï¸  Task 2 Warning: No tuning jobs found")
            return True
        
        completed_jobs = [j for j in jobs if j['HyperParameterTuningJobStatus'] == 'Completed']
        
        if completed_jobs:
            job = completed_jobs[0]
            print(f"âœ… Task 2 Passed: Tuning job completed, best objective: {job.get('BestTrainingJob', {}).get('FinalHyperParameterTuningJobObjectiveMetric', {}).get('Value', 'N/A')}")
        else:
            print(f"âœ… Task 2 Passed: {len(jobs)} tuning job(s) found (in progress)")
        
        return True
    except Exception as e:
        print(f"âš ï¸  Task 2 Warning: {e}")
        return True

def test_batch_transform_exists(sagemaker_client, student_id):
    """Task 3: Verify batch transform job"""
    try:
        response = sagemaker_client.list_transform_jobs(
            NameContains=student_id,
            MaxResults=10,
            SortBy='CreationTime',
            SortOrder='Descending'
        )
        jobs = response.get('TransformJobSummaries', [])
        
        if not jobs:
            print(f"âš ï¸  Task 3 Warning: No batch transform jobs found")
            return True
        
        completed_jobs = [j for j in jobs if j['TransformJobStatus'] == 'Completed']
        
        if completed_jobs:
            print(f"âœ… Task 3 Passed: {len(completed_jobs)} completed batch transform job(s)")
        else:
            print(f"âœ… Task 3 Passed: {len(jobs)} batch transform job(s) found")
        
        return True
    except Exception as e:
        print(f"âš ï¸  Task 3 Warning: {e}")
        return True

def test_model_registry(sagemaker_client, student_id):
    """Task 4: Verify models in Model Registry"""
    try:
        response = sagemaker_client.list_model_packages(
            MaxResults=10,
            SortBy='CreationTime',
            SortOrder='Descending'
        )
        
        model_packages = response.get('ModelPackageSummaryList', [])
        student_models = [m for m in model_packages if student_id in m.get('ModelPackageArn', '')]
        
        if student_models:
            print(f"âœ… Task 4 Passed: {len(student_models)} model(s) in registry")
            return True
        
        print(f"âš ï¸  Task 4 Warning: No models found in registry")
        return True
    except Exception as e:
        print(f"âš ï¸  Task 4 Warning: {e}")
        return True

def test_monitoring_schedules(sagemaker_client, student_id):
    """Task 5-6: Verify model monitoring schedules"""
    try:
        response = sagemaker_client.list_monitoring_schedules(
            MaxResults=20,
            SortBy='CreationTime',
            SortOrder='Descending'
        )
        
        schedules = response.get('MonitoringScheduleSummaries', [])
        student_schedules = [s for s in schedules if student_id in s.get('MonitoringScheduleName', '')]
        
        if not student_schedules:
            print(f"âš ï¸  Task 5-6 Warning: No monitoring schedules found")
            return True
        
        # Check for different monitoring types
        data_quality = [s for s in student_schedules if 'data' in s['MonitoringScheduleName'].lower()]
        model_quality = [s for s in student_schedules if 'model' in s['MonitoringScheduleName'].lower()]
        
        print(f"âœ… Task 5-6 Passed: {len(data_quality)} data quality, {len(model_quality)} model quality monitor(s)")
        return True
    except Exception as e:
        print(f"âš ï¸  Task 5-6 Warning: {e}")
        return True

def test_clarify_jobs(sagemaker_client, student_id):
    """Task 7: Verify SageMaker Clarify processing jobs"""
    try:
        response = sagemaker_client.list_processing_jobs(
            NameContains='clarify',
            MaxResults=10,
            SortBy='CreationTime',
            SortOrder='Descending'
        )
        
        jobs = response.get('ProcessingJobSummaries', [])
        student_jobs = [j for j in jobs if student_id in j.get('ProcessingJobName', '')]
        
        if student_jobs:
            print(f"âœ… Task 7 Passed: {len(student_jobs)} Clarify job(s) found")
            return True
        
        print(f"âš ï¸  Task 7 Warning: No Clarify jobs found")
        return True
    except Exception as e:
        print(f"âš ï¸  Task 7 Warning: {e}")
        return True

def test_multi_model_endpoint(sagemaker_client, student_id):
    """Task 8: Verify multi-model endpoint"""
    try:
        response = sagemaker_client.list_endpoints(
            NameContains=student_id,
            MaxResults=20,
            SortBy='CreationTime',
            SortOrder='Descending'
        )
        
        endpoints = response.get('Endpoints', [])
        
        for endpoint in endpoints:
            endpoint_name = endpoint['EndpointName']
            
            # Check endpoint configuration
            try:
                config_response = sagemaker_client.describe_endpoint_config(
                    EndpointConfigName=endpoint_name
                )
                
                # Check if it's a multi-model endpoint
                variants = config_response.get('ProductionVariants', [])
                for variant in variants:
                    if variant.get('ModelName', '').startswith('multi'):
                        print(f"âœ… Task 8 Passed: Multi-model endpoint found: {endpoint_name}")
                        return True
            except:
                pass
        
        print(f"âš ï¸  Task 8 Warning: No multi-model endpoints found")
        return True
    except Exception as e:
        print(f"âš ï¸  Task 8 Warning: {e}")
        return True

def test_lambda_functions(lambda_client, student_id):
    """Task 9: Verify Lambda functions for retraining"""
    try:
        response = lambda_client.list_functions(MaxItems=100)
        
        functions = response.get('Functions', [])
        student_functions = [f for f in functions if student_id in f.get('FunctionName', '')]
        retraining_functions = [f for f in student_functions if 'retrain' in f.get('FunctionName', '').lower()]
        
        if retraining_functions:
            print(f"âœ… Task 9 Passed: {len(retraining_functions)} retraining Lambda function(s) found")
            return True
        
        print(f"âš ï¸  Task 9 Warning: No retraining Lambda functions found")
        return True
    except Exception as e:
        print(f"âš ï¸  Task 9 Warning: {e}")
        return True

def test_codepipeline_exists(codepipeline_client, student_id):
    """Task 10: Verify CI/CD pipeline for ML"""
    try:
        response = codepipeline_client.list_pipelines()
        
        pipelines = response.get('pipelines', [])
        student_pipelines = [p for p in pipelines if student_id in p.get('name', '')]
        
        if student_pipelines:
            print(f"âœ… Task 10 Passed: {len(student_pipelines)} CodePipeline(s) found")
            return True
        
        print(f"âš ï¸  Task 10 Warning: No CodePipelines found")
        return True
    except Exception as e:
        print(f"âš ï¸  Task 10 Warning: {e}")
        return True

def test_output_files(student_id):
    """Verify output files exist"""
    expected_files = [
        f'pipeline-{student_id}.json',
        f'tuning-job-{student_id}.json',
        f'batch-transform-{student_id}.json'
    ]
    
    found_files = sum(1 for f in expected_files if os.path.exists(f))
    
    if found_files > 0:
        print(f"âœ… Output Files: {found_files}/{len(expected_files)} file(s) generated")
        return True
    
    print(f"âš ï¸  Output Files: No output files found")
    return True

def main():
    """Run all AI/ML Hard assessment tests"""
    print("=" * 60)
    print("AWS AI/ML Services - Hard Assessment - Validation")
    print("=" * 60)
    
    student_id = get_student_id()
    print(f"\nğŸ“ Student ID: {student_id}")
    print(f"ğŸ¤– Testing Advanced SageMaker Features\n")
    
    # Initialize AWS clients
    try:
        sagemaker_client = boto3.client('sagemaker')
        lambda_client = boto3.client('lambda')
        codepipeline_client = boto3.client('codepipeline')
    except Exception as e:
        print(f"âŒ Error initializing AWS clients: {e}")
        sys.exit(1)
    
    # Run tests
    results = []
    total_tasks = 10
    
    print("Starting validation...\n")
    
    # Task 1: SageMaker Pipeline
    results.append(test_pipeline_exists(sagemaker_client, student_id))
    
    # Task 2: Hyperparameter Tuning
    results.append(test_tuning_job_exists(sagemaker_client, student_id))
    
    # Task 3: Batch Transform
    results.append(test_batch_transform_exists(sagemaker_client, student_id))
    
    # Task 4: Model Registry
    results.append(test_model_registry(sagemaker_client, student_id))
    
    # Task 5-6: Monitoring
    results.append(test_monitoring_schedules(sagemaker_client, student_id))
    
    # Task 7: Clarify
    results.append(test_clarify_jobs(sagemaker_client, student_id))
    
    # Task 8: Multi-Model Endpoint
    results.append(test_multi_model_endpoint(sagemaker_client, student_id))
    
    # Task 9: Lambda Functions
    results.append(test_lambda_functions(lambda_client, student_id))
    
    # Task 10: CodePipeline
    results.append(test_codepipeline_exists(codepipeline_client, student_id))
    
    # Output files check
    results.append(test_output_files(student_id))
    
    # Calculate score
    passed = sum(results)
    score_percentage = (passed / total_tasks) * 100
    
    # Print summary
    print("\n" + "=" * 60)
    print("ASSESSMENT SUMMARY")
    print("=" * 60)
    print(f"Total Tasks: {total_tasks}")
    print(f"Passed: {passed}")
    print(f"Failed: {total_tasks - passed}")
    print(f"Score: {score_percentage:.1f}%")
    print("=" * 60)
    
    if score_percentage >= 70:
        print("\nğŸ‰ CONGRATULATIONS! You passed the AI/ML Hard assessment!")
        print("You've successfully demonstrated:")
        print("  âœ“ SageMaker Pipelines for MLOps")
        print("  âœ“ Hyperparameter tuning optimization")
        print("  âœ“ Batch transform for large-scale inference")
        print("  âœ“ Model Registry for versioning")
        print("  âœ“ Model monitoring (data & model quality)")
        print("  âœ“ SageMaker Clarify for bias detection")
        print("  âœ“ Multi-model endpoints")
        print("  âœ“ Automated retraining workflows")
        print("  âœ“ CI/CD pipelines for ML")
        sys.exit(0)
    else:
        print("\nâŒ Assessment not passed. Minimum score required: 70%")
        print("Please review the failed tasks and try again.")
        sys.exit(1)

if __name__ == "__main__":
    main()
