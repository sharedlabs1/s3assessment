# AWS Assessment â€” S3 + EMR (Easy)

Difficulty: ðŸŸ¢ Easy

Scenario
You will stage data in S3 and run a simple Spark job on EMR Serverless to process it.

Tasks (concise):

1. Create an S3 bucket named `s3-emr-<your-student-id>` and upload a CSV dataset into `input/`.
2. Create an EMR Serverless application named `emr-spark-<your-student-id>`.
3. Submit a simple Spark job that reads `s3://s3-emr-<your-student-id>/input/` and writes aggregated results to `s3://s3-emr-<your-student-id>/output/`.
4. Verify output files exist and contain aggregated rows (e.g., group-by count).
5. Clean up EMR job runs and stop/delete Serverless application if desired.

Notes
- Keep code short: a single PySpark script is sufficient.
- This README is intentionally brief â€” test participant's ability to implement the steps independent of detailed guidance.

Run tests

1. Install dependencies (locally or in CI):

```bash
pip install -r requirements.txt
```

2. Run validation (recommended):

```bash
python validate_tasks.py s3_emr_easy
# enter your student id when prompted
```

The validation helper triggers the repository GitHub Action which runs the checks and downloads `test_report.log` into the repository root. Use the GitHub Action run logs to debug failures.

Starter files (lightweight templates)

This repository includes small starter files you can edit as part of the assessment:

- `scripts/spark_job.py` â€” minimal PySpark job placeholder
- `scripts/bootstrap.sh` â€” example bootstrap action script
- `CLEANUP.md` â€” suggested cleanup steps

See the template: `scripts/spark_job.py` for the PySpark skeleton and TODO checklist.

Templates include short inline checklists marked with the token `TODO` â€” the validator looks for these to guide grading.

These are templates only (not solutions). Edit them to implement your job logic.